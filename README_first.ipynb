{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02f76e55-3d57-41bb-97e4-46531fc68e3d",
   "metadata": {},
   "source": [
    "# Training Hugging Face Models on an IPU using Paperspace Gradient\n",
    "\n",
    "These examples show you how to use Hugging Face models and train them on a Graphcore IPU using the Paperspace Gradient environment.\n",
    "\n",
    "You will learn how to use IPUs in a Jupyter-style notebook. You will be able to use IPUs to train large models including vision transformers (ViT) for images, BERT-Large for question answering, and text classification.\n",
    "\n",
    "## Graphcore Hugging Face models\n",
    "\n",
    "Hugging Face provides convenient access to pre-trained transformer models. The partnership between Hugging Face and Graphcore allows us to fine-tune and run these models on an IPU in Gradient.\n",
    "\n",
    "To help you get started, we have a some real-world examples including Stable Diffusion, speech recognition, natural language processing and image manipulation. These are based on [ðŸ¤— Optimum Graphcore](https://github.com/huggingface/optimum-graphcore), the interface between [ðŸ¤— Transformers](https://github.com/huggingface/transformers) and [Graphcore IPUs](https://www.graphcore.ai/products/ipu). Hugging Face provides convenient access to pre-trained transformer models. The partnership between Hugging Face and Graphcore allows us to run these models on the IPU.\n",
    "\n",
    "These worked examples will help you get started running your application on the IPU. They also demonstrate techniques such as data-parallelism and pipelining to increase perforce by using multiple IPUs.\n",
    "\n",
    "## Getting started\n",
    "\n",
    "No specific user setup is required to train IPU models on Gradient.\n",
    "\n",
    "## Natural language processing\n",
    "\n",
    "The [natural-language-processing](natural-language-processing) folder contains several notebooks for tasks such as answering questions, summarising text, and translation.\n",
    "\n",
    "* [Introduction to ðŸ¤— Optimum Graphcore: BERT Fine-tuning on IPUs](natural-language-processing/introduction_to_optimum_graphcore.ipynb) uses a BERT model to introduce the process of running on the IPU. This could be a good starting point to get familiar with the IPU, even if this isnâ€™t your main area of interest. It has a good introduction to pipelining an application across multiple IPUs.\n",
    "\n",
    "* The [sentiment analysis notebook](natural-language-processing/sentiment_analysis.ipynb) is another good starting point. It walks through the use of increasingly sophisticated models to classify text. It also shows how to run these applications on multiple IPUs to improve performance.\n",
    "\n",
    "* [Language Modelling from Scratch](natural-language-processing/language_modelling_from_scratch.ipynb) demonstrates how to train a ðŸ¤— Transformers model on a language modeling task.\n",
    "\n",
    "* Finally, the [external model notebook](natural-language-processing/external_model.ipynb) shows how you can train a model that is not directly supported by Optimum Graphcore or a ðŸ¤— Transformer.\n",
    "\n",
    "## Stable Diffusion\n",
    "\n",
    "The [Stable Diffusion](stable-diffusion) folder contains several ready to run examples of text-to-image, image-to-image and inpainting.\n",
    "\n",
    "## Speech recognition\n",
    "\n",
    "The [audio-processing](audio-processing) folder contains notebooks for automatic speech recognition.\n",
    "\n",
    "The [audio classification](audio-processing/audio_classification.ipynb) notebook shows how to fine-tune a multi-lingual model for automatic speech recognition using IPUs.\n",
    "\n",
    "There are two notebooks that use a wav2vec 2.0 model. The first for [fine-tuning the model](audio-processing/wav2vec2-fine-tuning-checkpoint.ipynb) and then for [running inference](audio-processing/wav2vec2-inference-checkpoint.ipynb) on the output of that. For more information, see the fine-tuning notebook.\n",
    "\n",
    "## Image classification\n",
    "\n",
    "The [image-processing](image-processing) folder contains an image classification example using the Vision Transformer (ViT) or ConvNeXT models\n",
    "\n",
    "## Useful tips\n",
    "\n",
    "Finally, the [useful-tips](useful-tips) folder contains information about how to make best use of the IPU resources. For example monitoring IPU use, releasing IPUs when you are not using them, and then re-attaching your model to the IPU when you start again.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
